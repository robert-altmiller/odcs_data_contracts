{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ed38a4-447a-4bf8-a530-8d2292d8c9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c67ab6-6263-43e8-8046-83c0530f3156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from datacontract.data_contract import DataContract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0094bbfd-cd1f-447b-9f98-429c417b268c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execute Data Contract DDL"
    }
   },
   "outputs": [],
   "source": [
    "def create_tables(yaml_file_path):\n",
    "    data_contract = DataContract(data_contract_file=yaml_file_path, spark=spark)\n",
    "    queries_ddl_list = data_contract.export(\"sql\")[:-1].split(\";\")\n",
    "    print(queries_ddl_list)\n",
    "    for query in queries_ddl_list:\n",
    "        query = f'''{query}'''\n",
    "        try:\n",
    "            spark.sql(query)\n",
    "        except Exception as e:\n",
    "            print(f\"{e}\\n\")\n",
    "            if \"[SCHEMA_NOT_FOUND]\" in str(e):\n",
    "                match = query.split(\"REPLACE TABLE\")[1].split(\"(\")[0].strip()\n",
    "                catalog = match.split(\".\")[0] if match else None\n",
    "                schema = match.split(\".\")[1] if match else None\n",
    "                spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "                print(f\"created schema {catalog}.{schema} successfully\\n\")\n",
    "                spark.sql(query)\n",
    "                continue\n",
    "        print(f\"COMPLETED RUNNING DDL QUERY:\\n{query}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "087116cd-2cc0-4788-82b6-35d4724541c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_general_data_quality_rules(table_name, columns=None):\n",
    "    \"\"\"\n",
    "    Generates a generic set of data quality SQL rules for a given data contract.\n",
    "\n",
    "    These rules include:\n",
    "    1. A row count check to ensure the table contains data.\n",
    "    2. A uniqueness check across specified columns to ensure no duplicate rows.\n",
    "    Args:\n",
    "        table_name (str): The name of the table for which to create rules.\n",
    "        columns (list, optional): List of column names to use for the duplicate check.\n",
    "                                  If None or empty, the duplicate rule will be skipped or invalid.\n",
    "    Returns:\n",
    "        list: A list of data quality rule dictionaries formatted for use in a data contract.\n",
    "    \"\"\"\n",
    "    partition_by_clause = \", \".join(columns) if columns else \"\"\n",
    "    \n",
    "    general_data_quality_rules = {\n",
    "        f\"{table_name}\": {\n",
    "            \"quality\": [\n",
    "                {\n",
    "                    \"type\": \"sql\",\n",
    "                    \"description\": f\"Ensures '{table_name}' table has data\",\n",
    "                    \"query\": f\"SELECT COUNT(*) FROM {table_name}\",\n",
    "                    \"mustBeGreaterThan\": 0\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Add duplicate check only if valid columns are provided\n",
    "    if partition_by_clause:\n",
    "        general_data_quality_rules[table_name][\"quality\"].append(\n",
    "            {\n",
    "                \"type\": \"sql\",\n",
    "                \"description\": f\"Ensure '{table_name}' table has no duplicate rows across all columns\",\n",
    "                \"query\": f\"\"\"\n",
    "                    SELECT COUNT(*)\n",
    "                    FROM (\n",
    "                        SELECT *, COUNT(*) OVER (PARTITION BY {partition_by_clause}) AS row_count\n",
    "                        FROM {table_name}\n",
    "                    ) AS subquery\n",
    "                    WHERE row_count > 1\n",
    "                \"\"\",\n",
    "                \"mustBe\": 0\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Clean up SQL formatting (flatten multi-line SQL to single-line strings)\n",
    "    for dq_rule in general_data_quality_rules[table_name][\"quality\"]:\n",
    "        dq_rule[\"query\"] = ' '.join(dq_rule[\"query\"].split())\n",
    "    return general_data_quality_rules[table_name][\"quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a849014-b37c-44e6-8a0b-6079d3640a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_custom_data_quality_rules(table_name, custom_dq_rules_input):\n",
    "    \"\"\"\n",
    "    Generates a custom set of data quality SQL rules for a given data contract table.\n",
    "    These rules are specific to the `customer` table and include:\n",
    "    1. A row count check to ensure the table does not exceed 100 records.\n",
    "    2. A null check to ensure all customers have an email.\n",
    "    3. A null check to ensure all customers have both first and last names.\n",
    "    Args:\n",
    "        table_name (str): The name of the table for which to generate custom rules.\n",
    "    Returns:\n",
    "        list: A list of data quality rule dictionaries formatted for use in a data contract.\n",
    "    Raises:\n",
    "        KeyError: If no custom rules are defined for the specified table.\n",
    "    \"\"\"\n",
    "\n",
    "    custom_data_quality_rules = {}\n",
    "    for rule in custom_dq_rules_input:\n",
    "        custom_data_quality_rules.update(rule)\n",
    "\n",
    "    if table_name not in custom_data_quality_rules:\n",
    "        raise KeyError(f\"No custom data quality rules defined for table: {table_name}\")\n",
    "    return custom_data_quality_rules[table_name][\"quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37037d0d-24cc-44d6-ba79-696b36fbe79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract properties from a table\n",
    "def get_table_properties(table):\n",
    "    \"\"\"\n",
    "    Extracts properties from a given table.\n",
    "\n",
    "    Args:\n",
    "        table (dict): The table definition containing fields.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of property dictionaries for the table.\n",
    "    \"\"\"\n",
    "    # Extract fields from the table\n",
    "    fields = table.get(\"fields\", [])\n",
    "    properties = []\n",
    "    \n",
    "    # Iterate over each field in the table\n",
    "    for field in fields:\n",
    "        property = {}\n",
    "        # Iterate over each key-value pair in the field\n",
    "        for key, value in field.items():\n",
    "            if key == \"tags\":\n",
    "                # Convert tags to the required format\n",
    "                property[\"tags\"] = [f\"{k}:{v}\" for k, v in value.items()] if value else []\n",
    "            elif key == \"type\":\n",
    "                # Set both logical and physical types to the field type\n",
    "                property[\"logicalType\"] = property[\"physicalType\"] = value\n",
    "            elif key == \"unique\":\n",
    "                # Set the unique property\n",
    "                property[key] = value\n",
    "            elif key == \"required\":\n",
    "                # Set the nullable property\n",
    "                property[key] = value\n",
    "            else:\n",
    "                # Set any other properties\n",
    "                property[key] = value if value else \"\"\n",
    "        # Add the property to the list of properties\n",
    "        properties.append(property)\n",
    "    # Return the list of properties\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "298e0c23-2123-4553-911c-bd32130256d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_data_quality_rules(table):\n",
    "    \"\"\"\n",
    "    Retrieves the data quality rules defined for a given table.\n",
    "    \n",
    "    Args:\n",
    "        table (dict): The table definition containing data quality rules.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of data quality rules for the table, defaulting to an empty list if not found.\n",
    "    \"\"\"\n",
    "    # Get the data quality rules from the table, defaulting to an empty list if not specified\n",
    "    return table.get(\"quality\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d06469-b6f9-477c-b9ff-ddce6ac6b491",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function to build the ODCS schema from a given schema"
    }
   },
   "outputs": [],
   "source": [
    "def build_odcs_schema(schema, odcs_schema):\n",
    "    \"\"\"\n",
    "    Builds the ODCS schema by extracting tables from the input schema and \n",
    "    converting them into the ODCS table format.\n",
    "\n",
    "    Args:\n",
    "        schema (dict): The input schema containing tables.\n",
    "        odcs_schema (list): The list to store the built ODCS schema.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract tables from the schema\n",
    "    schema_tables = schema.get(\"tables\", [])  # Get the tables from the schema, defaulting to an empty list if not found\n",
    "\n",
    "    # Iterate over each table in the schema\n",
    "    for table in schema_tables:\n",
    "        # Get the table name\n",
    "        table_name = table.get(\"name\", \"\")  # Get the table name, defaulting to an empty string if not found\n",
    "\n",
    "        # Get the table tags\n",
    "        table_tags = table.get(\"tags\", {})  # Get the table tags, defaulting to an empty dictionary if not found\n",
    "\n",
    "        # Extract properties from the table\n",
    "        properties = get_table_properties(table)  # Extract properties from the table using the get_table_properties function\n",
    "        table_quality_rules = get_table_data_quality_rules(table)  # Get the table quality rules using the get_table_data_quality_rules function\n",
    "        generic_quality_rules = get_general_data_quality_rules(table_name)  # Get the generic quality rules using the get_general_data_quality_rules function\n",
    "        quality_rules = generic_quality_rules + table_quality_rules  # Combine the generic and table quality rules\n",
    "\n",
    "        # Create the ODCS table object\n",
    "        odcs_table = {\n",
    "            # Get the logical type, defaulting to 'object' if not specified\n",
    "            \"logicalType\": table.get(\"logicalType\", \"object\"),  \n",
    "            # Get the table name\n",
    "            \"name\": table_name,  \n",
    "            # Get the physical name, defaulting to the table name if not specified\n",
    "            \"physicalName\": table.get(\"physicalName\", table_name),  \n",
    "            # Get the physical type, defaulting to 'object' if not specified\n",
    "            \"physicalType\": table.get(\"physicalType\", \"object\"),  \n",
    "            # Convert table tags to the required format\n",
    "            \"tags\": [f\"{k}:{v}\" for k, v in table_tags.items()] if table_tags else [],  \n",
    "            # Add the table properties\n",
    "            \"properties\": properties,  \n",
    "            \"quality\": quality_rules  # Add the combined quality rules\n",
    "        }\n",
    "        # Append the ODCS table to the schema\n",
    "        odcs_schema.append(odcs_table)  # Add the ODCS table to the odcs_schema list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40dd690e-c672-4fa2-83ac-54966c2c226a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function to convert  JSON schema to ODCS"
    }
   },
   "outputs": [],
   "source": [
    "def convert_json_to_odcs(json_file, odcs_template_file, host, environment, type = \"databricks\"):\n",
    "    \"\"\"\n",
    "    Converts JSON schema to ODCS (Open Data Catalog Specification) format.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): The path to the JSON file containing the schema.\n",
    "        odcs_template_file (str): The path to the ODCS template YAML file.\n",
    "        host (str): The host URL.\n",
    "        environment (str): The environment name.\n",
    "        type (str): environment type\n",
    "\n",
    "    Returns:\n",
    "        dict: The ODCS schema in dictionary format.\n",
    "    \"\"\"\n",
    "    # Initialize the schema JSON variable\n",
    "    schema_json = None\n",
    "    \n",
    "    # Load the JSON schema from the file\n",
    "    with open(json_file, \"r\") as file:\n",
    "        schema_json = json.load(file)\n",
    "\n",
    "    # Load the ODCS template from the YAML file\n",
    "    with open(odcs_template_file, \"r\") as file:\n",
    "        odcs_template = yaml.safe_load(file)\n",
    "\n",
    "    # Extract models from the schema JSON\n",
    "    models = schema_json.get(\"models\", [])\n",
    "    \n",
    "    # Initialize lists for servers, ODCS schema, and schema tags\n",
    "    servers = []\n",
    "    odcs_schema = []\n",
    "    stags = []\n",
    "\n",
    "    # Iterate over each model in the schema\n",
    "    for model in models:\n",
    "        # Extract catalog name and tags from the model\n",
    "        catalog_name = model.get(\"catalog\", \"\")\n",
    "        catalog_tags = model.get(\"tags\", {})\n",
    "\n",
    "        # Extract schema from the model\n",
    "        schema = model.get(\"schema\", {})\n",
    "        schema_name = schema.get(\"name\", \"\")\n",
    "        schema_tags = schema.get(\"tags\")\n",
    "\n",
    "        # Convert schema tags to key-value pairs if present\n",
    "        if schema_tags is not None:\n",
    "            # Iterate over each tag and append to the schema tags list\n",
    "            for k, v in schema_tags.items():\n",
    "                stags.append(f\"{k}:{v}\")\n",
    "\n",
    "        # Create a server entry for the model\n",
    "        servers.append(\n",
    "            {\n",
    "                \"catalog\": catalog_name,\n",
    "                \"host\": host,\n",
    "                \"type\": type,\n",
    "                \"schema\": schema_name,\n",
    "                \"server\": environment,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Build the ODCS schema for the model\n",
    "        build_odcs_schema(schema, odcs_schema)\n",
    "\n",
    "    # Update the ODCS template with the built schema, tags, and servers\n",
    "    odcs_template[\"schema\"] = odcs_schema\n",
    "    odcs_template[\"tags\"] = stags\n",
    "    odcs_template[\"servers\"] = servers\n",
    "    \n",
    "    # Return the updated ODCS template\n",
    "    return odcs_template"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "contract_first_helpers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
